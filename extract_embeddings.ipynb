{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Initialize the BETO tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "model = TFBertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(file_path, word, num_samples):\n",
    "    counter = 0\n",
    "    embeddings = []\n",
    "    sentences = []\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        lines = random.sample(file.read().splitlines(), k=num_samples)\n",
    "        print(f\"Extracted {num_samples} lines\")\n",
    "        lines = list(set([line.lower() for line in lines if len(line) < 512 and word in line]))\n",
    "        print('Cleaning samples...')\n",
    "        print(f'Total samples after cleaning: {len(lines)}')\n",
    "\n",
    "    print('Extracting embeddings...')\n",
    "\n",
    "    # Iterate over every match and extract the embedding for the given word\n",
    "    for line in lines:\n",
    "        \n",
    "        # Print out the progress so far\n",
    "        if counter % 100 == 0 and counter != 0:\n",
    "            print(f'Processed {counter} lines...')\n",
    "\n",
    "        # Tokenize the sentence and convert it to BERT input format\n",
    "        inputs = tokenizer.encode_plus(line, add_special_tokens=True, return_tensors='tf')\n",
    "        inputs['token_type_ids'] = tf.zeros_like(inputs['input_ids'])\n",
    "\n",
    "        # Feed the inputs to BERT and extract the output hidden states\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        # Extract the embedding for the word from the final hidden state\n",
    "        try:\n",
    "            embedding = hidden_states[-1][0][inputs['input_ids'][0].numpy().tolist().index(tokenizer.vocab[word])].numpy()\n",
    "        except ValueError:\n",
    "            print('The token {} is not present in the input text.'.format(word))\n",
    "            embedding = None\n",
    "\n",
    "        if embedding is not None:\n",
    "            embedding = hidden_states[-1][0][inputs['input_ids'][0].numpy().tolist().index(tokenizer.vocab[word])].numpy()\n",
    "            # Add the embedding to the list\n",
    "            sentences.append(line)\n",
    "            embeddings.append(embedding)\n",
    "            counter += 1\n",
    "    return embeddings, sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 5600 lines\n",
      "Cleaning samples...\n",
      "Total samples after cleaning: 5339\n",
      "Extracting embeddings...\n",
      "Processed 100 lines...\n",
      "Processed 200 lines...\n",
      "Processed 300 lines...\n",
      "Processed 400 lines...\n",
      "Processed 500 lines...\n",
      "Processed 600 lines...\n",
      "Processed 700 lines...\n",
      "Processed 800 lines...\n",
      "Processed 900 lines...\n",
      "Processed 1000 lines...\n",
      "Processed 1100 lines...\n",
      "Processed 1200 lines...\n",
      "Processed 1300 lines...\n",
      "Processed 1400 lines...\n",
      "Processed 1500 lines...\n",
      "Processed 1600 lines...\n",
      "Processed 1700 lines...\n",
      "Processed 1800 lines...\n",
      "Processed 1900 lines...\n",
      "Processed 2000 lines...\n",
      "Processed 2100 lines...\n",
      "Processed 2200 lines...\n",
      "Processed 2300 lines...\n",
      "Processed 2400 lines...\n",
      "Processed 2500 lines...\n",
      "Processed 2600 lines...\n",
      "Processed 2700 lines...\n",
      "Processed 2800 lines...\n",
      "Processed 2900 lines...\n",
      "Processed 3000 lines...\n",
      "Processed 3100 lines...\n",
      "Processed 3200 lines...\n",
      "Processed 3300 lines...\n",
      "Processed 3400 lines...\n",
      "Processed 3500 lines...\n",
      "Processed 3600 lines...\n",
      "Processed 3700 lines...\n",
      "Processed 3800 lines...\n",
      "Processed 3900 lines...\n",
      "Processed 4000 lines...\n",
      "Processed 4100 lines...\n",
      "Processed 4200 lines...\n",
      "Processed 4300 lines...\n",
      "Processed 4400 lines...\n",
      "Processed 4500 lines...\n",
      "Processed 4600 lines...\n",
      "Processed 4700 lines...\n",
      "Processed 4800 lines...\n",
      "Processed 4900 lines...\n",
      "Processed 5000 lines...\n",
      "Processed 5100 lines...\n",
      "Processed 5200 lines...\n",
      "Processed 5300 lines...\n"
     ]
    }
   ],
   "source": [
    "realmente_embeddings, realmente_sentences = extract_embeddings(\"realmente.txt\", 'realmente', 5600)\n",
    "\n",
    "# Save the embeddings to a file\n",
    "embeddings_array = np.array(realmente_embeddings)\n",
    "np.save('realmente_embeddings.npy', embeddings_array)\n",
    "\n",
    "# Save the sentences to a file\n",
    "with open('realmente_embeddings_sentences.txt', 'w', encoding='utf-8') as file:\n",
    "    file.writelines('\\n'.join(realmente_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1000 lines\n",
      "Cleaning samples...\n",
      "Total samples after cleaning: 974\n",
      "Extracting embeddings...\n",
      "Processed 100 lines...\n",
      "Processed 200 lines...\n",
      "The token muy is not present in the input text.\n",
      "Processed 300 lines...\n",
      "Processed 400 lines...\n",
      "Processed 500 lines...\n",
      "Processed 600 lines...\n",
      "Processed 700 lines...\n",
      "Processed 800 lines...\n",
      "Processed 900 lines...\n"
     ]
    }
   ],
   "source": [
    "muy_embeddings, muy_sentences = extract_embeddings(\"muy.txt\", 'muy', 1000)\n",
    "\n",
    "# Save the embeddings to a file\n",
    "muy_embeddings_array = np.array(muy_embeddings)\n",
    "np.save('muy_embeddings.npy', muy_embeddings_array)\n",
    "\n",
    "# Save the sentences to a file\n",
    "with open('muy_embeddings_sentences.txt', 'w', encoding='utf-8') as muy_file:\n",
    "    muy_file.writelines('\\n'.join(muy_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1000 lines\n",
      "Cleaning samples...\n",
      "Total samples after cleaning: 948\n",
      "Extracting embeddings...\n",
      "Processed 100 lines...\n",
      "Processed 200 lines...\n",
      "Processed 300 lines...\n",
      "Processed 400 lines...\n",
      "Processed 500 lines...\n",
      "Processed 600 lines...\n",
      "Processed 700 lines...\n",
      "Processed 800 lines...\n",
      "Processed 900 lines...\n"
     ]
    }
   ],
   "source": [
    "rapidamente_embeddings, rapidamente_sentences = extract_embeddings(\"rapidamente.txt\", 'rÃ¡pidamente', 1000)\n",
    "\n",
    "# Save the embeddings to a file\n",
    "rapidamente_embeddings_array = np.array(rapidamente_embeddings)\n",
    "np.save('rapidamente_embeddings.npy', rapidamente_embeddings_array)\n",
    "\n",
    "# Save the sentences to a file\n",
    "with open('rapidamente_embeddings_sentences.txt', 'w', encoding='utf-8') as rapid_file:\n",
    "    rapid_file.writelines('\\n'.join(rapidamente_sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
